{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ah2OTtHRjf1i",
    "outputId": "3883c391-a70e-4dec-9927-43ee36d6d759",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.0.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
      "time: 2.6 ms (started: 2021-06-04 06:55:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-autotime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfXhbvA8SBkt",
    "outputId": "f018bbe8-b098-4c58-a214-e560eee8706d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /root/.local/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
      "time: 2.94 s (started: 2021-06-04 06:55:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PGoS7SojpuS",
    "outputId": "14bc77d1-7540-453f-e810-4f78bd8e8df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "time: 3.23 ms (started: 2021-06-04 06:55:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0z_s4GpwOZ8U",
    "outputId": "87dd3b4c-d268-43d2-9099-175f910d558e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 801 ms (started: 2021-06-04 06:55:41 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag,pos_tag_sents\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "bde685f3-e95a-48a7-ed80-47702f3e6c5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 221 ms (started: 2021-06-04 06:55:42 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JOWJqE9Pq5V",
    "outputId": "05a298f4-8ab8-486d-8e1a-6da13b5a6e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.32 ms (started: 2021-06-04 06:55:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVD9Q3AxOZ8V",
    "outputId": "6a102ddb-2a08-41b1-d9bc-743523069a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.08 s (started: 2021-06-04 06:55:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "p = 'some_directory'\n",
    "df = pd.read_csv(os.path.join(p,'/content/drive/MyDrive/colab data/reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "92c3e8d4-2a1e-4218-a2e2-b8e8dc39f23f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id  ...                                           comments\n",
       "0        2818  ...  Daniel is really cool. The place was nice and ...\n",
       "1        2818  ...  Daniel is the most amazing host! His place is ...\n",
       "2        2818  ...  We had such a great time in Amsterdam. Daniel ...\n",
       "3        2818  ...  Very professional operation. Room is very clea...\n",
       "4        2818  ...  Daniel is highly recommended.  He provided all...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.7 ms (started: 2021-06-04 06:55:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "90024acb-7ead-4184-ad2c-a7e42bff4172"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.85 ms (started: 2021-06-04 06:55:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7jF_XXsQYgK",
    "outputId": "1b1bebc4-02df-4842-f325-1faa1c3a9cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.9 ms (started: 2021-06-04 06:55:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def process_reviews(df):\n",
    "  ''' This function perform task of tokenizing ,tagging and transform the tagged word into lower case.\n",
    "    argument = DataFrame\n",
    "    return = DataFrame with three additional columns''' \n",
    "  # word tokenizing\n",
    "  df['tokenized'] = df['comments'].apply(word_tokenize)\n",
    "  # tagging using pos_tag\n",
    "  tag = []\n",
    "  for comment in df.comments:\n",
    "    tag.append(pos_tag(comment.translate(str.maketrans('', '', string.punctuation)).split()))\n",
    "  df[\"tagged\"] = tag\n",
    "  # converting all the tagged words to lower to reduce memory usuage.\n",
    "  lower_tag = []\n",
    "  for tag in df.tagged:\n",
    "    lwr_tag = []\n",
    "    for word in tag:\n",
    "      wrd = (word[0].lower(), word[1])\n",
    "      lwr_tag.append(wrd)\n",
    "    lower_tag.append(lwr_tag)\n",
    "  df[\"lower_tagged\"] = lower_tag\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGYB8gx5Qq-P",
    "outputId": "3cb87baa-afb4-4af0-e501-2675fe671a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 4s (started: 2021-06-04 06:55:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df = process_reviews(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAg6VRwdQQmg",
    "outputId": "fdb9eb33-3d8c-4a42-bf92-24f2775c3ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 50.4 ms (started: 2021-06-04 07:18:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(df):\n",
    "  ''' This function generate two list, first one contains most common 1000 nouns and second one contains\n",
    "  most common 1000 verb/adjective.\n",
    "  argument: DataFrame\n",
    "  return: two lists.'''\n",
    "  new_list=[]\n",
    "  new_list1=[]\n",
    "  for i in range(len(df.tagged)):\n",
    "    x=df[\"lower_tagged\"][i]\n",
    "    new_list.append(x)\n",
    "  for j in range(len(new_list)):\n",
    "    t=new_list[j]\n",
    "    for k in range(len(t)):\n",
    "      p=new_list[j][k]\n",
    "      new_list1.append(p)\n",
    "  noun_list =[]\n",
    "  verb_list = []\n",
    "  noun_in = ['NNP','NNS','NNPS','NN'] # list of noun tags to check whether tag is noun or not\n",
    "  verb_in = ['JJS','JJ','JJR']        # list of verb/adjective tags to check whether tag is verb/adjective or not\n",
    "  # foor loop to check tag type.\n",
    "  for tok,tag in new_list1:\n",
    "    if tag in noun_in:\n",
    "      noun_list.append(tok)           # if tag is noun it will be added to noun list\n",
    "    elif tag in verb_in:\n",
    "      verb_list.append(tok)           # if tag is verb/adjective it will be added to adjective\n",
    "\n",
    "  # using Counter function to count the the occurance of a word.\n",
    "  noun_count = Counter(noun_list)\n",
    "  verb_count = Counter(verb_list)\n",
    "  noun_sorted = noun_count.most_common()\n",
    "  verb_sorted = verb_count.most_common()\n",
    "  new_verb=[]\n",
    "  new_noun=[]\n",
    "  for i in tqdm(range(len(verb_sorted))):\n",
    "    L=verb_sorted[i][0]\n",
    "    new_verb.append(L)\n",
    "  for i in tqdm(range(len(noun_sorted))):\n",
    "    L=noun_sorted[i][0]\n",
    "    new_noun.append(L)\n",
    "  # removing puntuation who got tagged as noun or verb/adjective\n",
    "  new_noun = [''.join(c for c in s if c not in string.punctuation) for s in new_noun]\n",
    "  new_noun = [s for s in new_noun if s]\n",
    "  new_verb = [''.join(c for c in s if c not in string.punctuation) for s in new_verb]\n",
    "  new_verb = [s for s in new_verb if s]\n",
    "  # to keep unique values in both the lists\n",
    "  final_noun=[]\n",
    "  for i in new_noun:\n",
    "    if i not in new_verb:\n",
    "      final_noun.append(i)\n",
    "  # extracting only top 1000 words in both the vocabs\n",
    "  cent_vocab = final_noun[:1000]\n",
    "  cont_vocab = new_verb[:1000]\n",
    "  return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_R5l4IVSk9-",
    "outputId": "fc09bc1c-e47d-4e52-dab5-589220801cdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41728/41728 [00:00<00:00, 1125124.83it/s]\n",
      "100%|██████████| 169268/169268 [00:00<00:00, 1216996.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 4s (started: 2021-06-04 07:18:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddnfCbQWRd5R",
    "outputId": "8155d3b1-76bb-4058-b14d-2e2edead5b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.4 ms (started: 2021-06-04 07:20:54 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  '''This function get_coocs(df, center_vocab, context_vocab) which takes as \n",
    "  argument: the DataFrame generated in step 1, and the lists generated in step 2  \n",
    "  returns a dictionary of dictionaries, of the form in the example below\n",
    "     ‘A big restaurant served delicious food in big dishes’\n",
    "     {‘restaurant’: {‘big’: 2, ‘served’:1, ‘delicious’:1}} '''\n",
    "  # we are taking context as a full review\n",
    "  coocs = {}\n",
    "  #dataframe comments into list as reviews\n",
    "  df[\"lower_tokenized\"]=df.comments.apply(str.lower).apply(word_tokenize) # lower casing the comment section and applying word tokenizing to reach each word using for loop.\n",
    "  reviews = df[\"lower_tokenized\"].to_list() # storing complete lower_tokeized column into reviews as list.\n",
    "  #First loop in cent_vocab\n",
    "  for cent in tqdm(cent_vocab):\n",
    "    #context coocs dictionary\n",
    "    cont_coocs = defaultdict(int)\n",
    "    #Second loop in reviews\n",
    "    for review in reviews:\n",
    "      #check the center vocab  in review or not\n",
    "      if cent in review:\n",
    "        #3rd loop in cont_vocab\n",
    "        for cont in cont_vocab:\n",
    "          #check the context vocab  in review or not\n",
    "          if cont in review:\n",
    "            #make a dictionary occurrence for context vocab\n",
    "            if cont in cont_coocs:\n",
    "              cont_coocs[cont] += 1\n",
    "            else:\n",
    "              cont_coocs[cont] = 1\n",
    "          else:\n",
    "            cont_coocs[cont] = 0\n",
    "          #store the center key with value as context dictionry\n",
    "      coocs[cent] = cont_coocs \n",
    "  return coocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QP8C6Zexo44J",
    "outputId": "aebef18a-9f1f-4cb6-806b-9888df0447f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [40:00<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44min 42s (started: 2021-06-04 07:20:54 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6WuM5U7RsBJ",
    "outputId": "44a62db4-f33a-4f57-d1ca-fa6406c0b95b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.8 ms (started: 2021-06-04 08:05:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "  ''' This function takes dictionary of dictionaries as argument and return a DataFrame'''\n",
    "  coocdf = pd.DataFrame.from_dict(coocs,orient= 'index',dtype='Int64').fillna(0)\n",
    "  return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwAflxldSrbg",
    "outputId": "87615108-92f3-4902-aa68-b223e37c34dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.17 s (started: 2021-06-04 08:05:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frTTs7-eSFHv",
    "outputId": "2e55dd75-5756-4629-ecab-1bfc31756d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11 ms (started: 2021-06-04 08:05:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def cooc2pmi(df):\n",
    "  ''' This function takes step 4 DataFrame as argunment and return new Dataframe with PMI score instead of raw co-occurence count.'''\n",
    "  row_totals = df.sum(axis=1).astype(float)         # take the total sum of all rows in dataframe\n",
    "  prob_cols_given_row = (df.T / row_totals).T       # calculating the probability of each index against total sum of rows\n",
    "  col_totals = df.sum(axis=0).astype(float)         # calculating sum of all rows.\n",
    "  prob_of_cols = col_totals / sum(col_totals)       # calculating the probability of each index against total sum of columns.\n",
    "  ratio = prob_cols_given_row / prob_of_cols        # calculating ratio\n",
    "  ratio[ratio==0] = 0.00001                         # replacing ratios that have zero value with 0.00001 to avoid mathematical error.\n",
    "  pmidf = np.log(ratio)                             # calculating log of ratio using numpy library log function\n",
    "  pmidf[pmidf < 0] = 0\n",
    "  pmidf = pmidf.fillna(0.00001)\n",
    "  return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGftXjXRSuQw",
    "outputId": "332beca2-7293-41d2-fb8d-af756fb2f0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 726 ms (started: 2021-06-04 08:05:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlKUP9SgSXlL",
    "outputId": "e58524ab-3c20-42e5-e4e5-b1768ba79693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 ms (started: 2021-06-04 08:05:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "  ''' This function takes PMI score filled in Dataframe,center_word and an optional N argument with default value 10 as input \n",
    "      return a list of N strings in order of their PMI score with the center_word'''\n",
    "  top_words = df[center_word].sort_values(ascending = False).head(N) # finding the top N PMI score with there index values.\n",
    "  top_words= list(top_words.index.values) # storing index values in a list\n",
    "  return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1I038zG1Sw62",
    "outputId": "fb100a0c-b01a-4940-a4c4-47e3fb1d4f5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maker',\n",
       " 'mornings',\n",
       " 'cup',\n",
       " 'stroopwaffels',\n",
       " 'pods',\n",
       " 'difficulties',\n",
       " 'office',\n",
       " 'sanders',\n",
       " 'inconvenience',\n",
       " 'ducks']"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.63 ms (started: 2021-06-04 08:05:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "topk(pmidf,'coffee')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Part_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
